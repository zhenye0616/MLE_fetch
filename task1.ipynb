{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Sentence Transformer Implementation\n",
    "\n",
    "\n",
    "Implement a sentence transformer model using any deep learning framework of your choice.This model should be able to encode input sentences into fixed-length embeddings. Test yourimplementation with a few sample sentences and showcase the obtained embeddings.Describe any choices you had to make regarding the model architecture outside of the transformer backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizerFast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding the sentence inputs to the sentence transformer, we first have to tokenize the sentences and encode them, Here my choice was to use pretrained BERT as my backbone.\n",
    "\n",
    "get_tokenizer function simply loads the right HuggingFace tokenizer for the chosen BERT variant. \n",
    "\n",
    "encode_sentences then takes raw strings, applies that tokenizer to produce token IDs and an attention mask, pads every sequence in the batch up to its longest length, truncates anything beyond max_length, returns PyTorch tensors, and immediately moves them onto your target device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Tokenization & Encoding\n",
    "def get_tokenizer(model_name=\"bert-base-uncased\"):\n",
    "    return BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "def encode_sentences(tokenizer, sentences, max_length=128, device=\"cpu\"):\n",
    "    encoded = tokenizer(\n",
    "        sentences,\n",
    "        padding=\"longest\",     # pad to longest in batch\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return encoded[\"input_ids\"].to(device), encoded[\"attention_mask\"].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Sentence Transformer Model\n",
    "class BertSentenceTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_name=\"bert-base-uncased\",\n",
    "                 pooling: str = \"mean\"   # options: \"cls\", \"mean\", \"max\"\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.backbone = BertModel.from_pretrained(model_name)\n",
    "        self.pooling = pooling.lower()\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 1) get all token embeddings\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        token_embeddings = outputs.last_hidden_state  # (B, T, H)\n",
    "        cls_embedding    = outputs.pooler_output      # (B, H)\n",
    "\n",
    "        # 2) apply pooling\n",
    "        if self.pooling == \"cls\":\n",
    "            sent_emb = cls_embedding\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            # mask out padding tokens\n",
    "            mask = attention_mask.unsqueeze(-1).float()  # (B, T, 1)\n",
    "            sum_embeddings = (token_embeddings * mask).sum(dim=1)\n",
    "            lengths = mask.sum(dim=1).clamp(min=1e-9)\n",
    "            sent_emb = sum_embeddings / lengths\n",
    "\n",
    "        elif self.pooling == \"max\":\n",
    "            mask = attention_mask.unsqueeze(-1).bool()\n",
    "            # set padding tokens to very large negative so they don’t affect max\n",
    "            token_embeddings[~mask] = -1e9\n",
    "            sent_emb = token_embeddings.max(dim=1).values\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling: {self.pooling}\")\n",
    "\n",
    "\n",
    "        return sent_emb  # (B, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\n",
      " Sushi is the best food\n",
      "Embeddings shape: torch.Size([768])\n",
      "Sentence:\n",
      " lakers is winning this year\n",
      "Embeddings shape: torch.Size([768])\n",
      "Sentence:\n",
      " what is the capital of US\n",
      "Embeddings shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# 3) Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # a) prepare data\n",
    "    sentences = [\n",
    "        \"Sushi is the best food\",\n",
    "        \"lakers is winning this year\",\n",
    "        \"what is the capital of US\"\n",
    "    ]\n",
    "    tokenizer = get_tokenizer(\"bert-base-uncased\")\n",
    "    input_ids, attention_mask = encode_sentences(tokenizer, sentences, device=device)\n",
    "\n",
    "    model = BertSentenceTransformer(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        pooling=\"mean\"\n",
    "    ).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(input_ids, attention_mask)\n",
    "        for sentence, embed in zip(sentences, embeddings):\n",
    "            print('Sentence:\\n',sentence)\n",
    "            print(\"Embeddings shape:\", embed.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
