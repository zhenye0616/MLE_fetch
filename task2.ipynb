{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Multi-Task Learning Expansion\n",
    "Expand the sentence transformer to handle a multi-task learning setting.\n",
    "1. Task A: Sentence Classification – Classify sentences into predefined classes (you can\n",
    "make these up).\n",
    "2. Task B: [Choose another relevant NLP task such as Named Entity Recognition,\n",
    "Sentiment Analysis, etc.] (you can make the labels up)\n",
    "Describe the changes made to the architecture to support multi-task learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#              ┌────────────────────┐\n",
    "#              │  Tokenizer + BERT  │  ←—— all weights shared & fine‑tuned\n",
    "#              └────────────────────┘\n",
    "#                         │\n",
    "#       ┌─────────────────┴─────────────────┐\n",
    "#       │                                   │\n",
    "#       ▼                                   ▼\n",
    "#   Task A                               Task B\n",
    "# ┌─────────────────┐               ┌────────────────────┐\n",
    "# │ Sentence Head   │               │ Token Head (NER)   │\n",
    "# │ (CLS‑pool →     │               │ (per‑token softmax │\n",
    "# │  Dropout → Lin) │               │  or CRF)           │\n",
    "# └─────────────────┘               └────────────────────┘\n",
    "#       │                                   │\n",
    "#       ▼                                   ▼\n",
    "#    sent_logits                       token_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBert(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_name: str = \"bert-base-uncased\",\n",
    "                 num_sent_labels: int = 5,\n",
    "                 num_token_labels: int = 4,\n",
    "                 pooling: str = \"cls\"   # for the sentence head\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        # 1) Shared encoder\n",
    "        self.backbone = BertModel.from_pretrained(model_name)\n",
    "        hidden_size    = self.backbone.config.hidden_size\n",
    "        self.pooling   = pooling.lower()\n",
    "\n",
    "        # 2) Task A: sentence‐level classification head\n",
    "        self.sent_dropout = nn.Dropout(0.1)\n",
    "        self.sent_classifier = nn.Linear(hidden_size, num_sent_labels)\n",
    "\n",
    "        # 3) Task B: token‐level classification head (NER)\n",
    "        self.token_dropout = nn.Dropout(0.1)\n",
    "        self.token_classifier = nn.Linear(hidden_size, num_token_labels)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: torch.Tensor):\n",
    "        # Shared forward pass\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        token_embs = outputs.last_hidden_state  # (B, T, H)\n",
    "        cls_emb    = outputs.pooler_output      # (B, H)\n",
    "\n",
    "        # ——— Task A: Sentence Classification ———\n",
    "        if self.pooling == \"cls\":\n",
    "            sent_repr = cls_emb\n",
    "        elif self.pooling == \"mean\":\n",
    "            mask = attention_mask.unsqueeze(-1).float()\n",
    "            summed = (token_embs * mask).sum(dim=1)\n",
    "            lengths = mask.sum(dim=1).clamp(min=1e-9)\n",
    "            sent_repr = summed / lengths\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling: {self.pooling}\")\n",
    "\n",
    "        sent_logits = self.sent_classifier(self.sent_dropout(sent_repr))\n",
    "        # shape → (B, num_sent_labels)\n",
    "\n",
    "        # ——— Task B: Named Entity Recognition ———\n",
    "        # we apply token‐level dropout, then project each token to NER label space\n",
    "        token_logits = self.token_classifier(self.token_dropout(token_embs))\n",
    "        # shape → (B, T, num_token_labels)\n",
    "\n",
    "        return {\n",
    "            \"sent_logits\": sent_logits,\n",
    "            \"token_logits\": token_logits\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle_fetch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
